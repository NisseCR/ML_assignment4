{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMSxpJAkqYzk"
   },
   "source": [
    "# Assignment 4: Explainability\n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2023, Utrecht University*\n",
    "\n",
    "Total points: 10 (100%)\n",
    "\n",
    "Deadline: Friday 3 November, 23:59\n",
    "\n",
    "**Write your names and student numbers here: ___**\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moyaViIx8WzS"
   },
   "source": [
    "## Installation\n",
    "\n",
    "For this assignment, we are going to use the following Python packages:\n",
    "\n",
    "matplotlib, pandas, statsmodels, interpret, scikit-learn, openpyxl and graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "id": "6EaC6P7RqXOh",
    "ExecuteTime": {
     "end_time": "2023-10-30T13:13:55.908171700Z",
     "start_time": "2023-10-30T13:13:55.854063900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installing packages\n",
    "# !pip install graphviz\n",
    "# !pip install matplotlib pandas numpy statsmodels interpret scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeSC0_WEpY0k"
   },
   "source": [
    "## Downloading the data\n",
    "We are going to use the combined cycle power plant dataset. This dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. We have the following features: hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V). We will train ML models to predict the net hourly electrical energy output (EP) of the plant.\n",
    "\n",
    "For a detailed description, see: [[Description](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)]\n",
    "\n",
    "We first need to download and prepare data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "id": "fleSmPrE7UMT",
    "ExecuteTime": {
     "end_time": "2023-10-30T13:13:55.908171700Z",
     "start_time": "2023-10-30T13:13:55.859003500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download and unzip data\n",
    "\n",
    "# Windows users: download the data from https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\n",
    "# and unzip the file manually in the same folder as the python notebook\n",
    "\n",
    "# Note: this cell works fine on Linux based systems and Google Colab\n",
    "# If you run it on a Windows machine, you will get an error (...'wget' is not recognized as an internal or external command...)\n",
    "# !wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\n",
    "# !unzip CCPP.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQpW5C3Sg9YA"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "We split the data into training (first 5000 instances) and validation (the subsequent 2000) and test (the last 2568) sets. We will use the training set to train a model, and validation set to optimize the model hyper-parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "id": "JycjPmn_7p41",
    "ExecuteTime": {
     "end_time": "2023-10-30T13:13:55.940194100Z",
     "start_time": "2023-10-30T13:13:55.872333800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# global variables\n",
    "DATA_FILENAME = 'CCPP/Folds5x2_pp.xlsx'\n",
    "FEATURE_NAMES = ['AT', 'V', 'AP', 'RH']\n",
    "LABEL_NAME = 'PE'\n",
    "# Load the data from the excel file\n",
    "def load_data():\n",
    "    def split_feature_label(data_set):\n",
    "        features = data_set[FEATURE_NAMES]\n",
    "        labels = data_set[LABEL_NAME]\n",
    "        return features, labels\n",
    "\n",
    "    data = pd.read_excel(DATA_FILENAME)\n",
    "    train_set, dev_set, test_set = data[:5000], data[5000: 7000], data[7000:]\n",
    "\n",
    "    train_features, train_labels = split_feature_label(train_set)\n",
    "    dev_features, dev_labels = split_feature_label(dev_set)\n",
    "    test_features, test_labels = split_feature_label(test_set)\n",
    "\n",
    "    return train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels\n",
    "\n",
    "\n",
    "# preprocess (by z-normalization) the data for the regression task\n",
    "# return the normalized feature sets and corresponding target variables \n",
    "def prepare_load_regression_data():\n",
    "    train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(train_features)\n",
    "\n",
    "    print(train_features)\n",
    "\n",
    "    train_features = pd.DataFrame(data=scaler.transform(train_features), columns=FEATURE_NAMES)\n",
    "    dev_features = pd.DataFrame(data=scaler.transform(dev_features), columns=FEATURE_NAMES)\n",
    "    test_features = pd.DataFrame(data=scaler.transform(test_features), columns=FEATURE_NAMES)\n",
    "\n",
    "    return train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QabF2JOdMTI4"
   },
   "source": [
    "## Training and Interpreting a Linear Regression Model\n",
    "\n",
    "**Q1**. (10)% Train a linear regression model (we recommend the statsmodels package) and report $R^2$ (goodness of fit) statistic.\n",
    "\n",
    "For model interpretability, provide for each feature (+ the bias variable) the following in tabular format: \n",
    "* Weight estimates\n",
    "* SE (standard error of estimates) \n",
    "* T statistics\n",
    "\n",
    "*Answer*\n",
    "Our model has an $R^2$ of 0.929. Tabular data of weights, SE and T statistics can be found in the code cell below.\n",
    "\n",
    "Further Questions regarding the linear model (answers to be included in the notebook): \n",
    "\n",
    "**Q2**. (10%) Which three features are the most important?\n",
    "\n",
    "*Answer*\n",
    "The most important features can be obtained by looking at the highest absolute value of the T-statistic (exluding our bias feature):\n",
    "1. AT\n",
    "2. RH\n",
    "3. V\n",
    "\n",
    "**Q3**. (10%) How does the gas turbine energy yield (EP) change with unit (one degree C) increase of the ambient temperature given that all other feature values remain the same? (Note: Here you should consider whether you use the original or z-normalized features to train your linear model.)\n",
    "\n",
    "*Answer*\n",
    "EP changes by $\\approx$ -1.9999 with unit change of AT.\n",
    "\n",
    "**Q4**. (10%) Show bar graph illustrations of the feature effects for the first two development set instances.\n",
    "\n",
    "*Answer*\n",
    "Bar plots and dev instances are printed in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B91BszFhMStw",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-30T13:13:55.872333800Z"
    }
   },
   "outputs": [],
   "source": [
    "# We recommend the statsmodels package\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Hint, by default this sm does not include the bias/offset term w_0\n",
    "# thus, you should add it yourself using sm.add_constant()\n",
    "\n",
    "## Q1\n",
    "# Linear regression\n",
    "# Get the preprocessed data for linear regression modeling\n",
    "train_features, train_labels, dev_features, dev_labels, test_features, test_labels, scaler = prepare_load_regression_data()\n",
    "\n",
    "# Add w0 constant\n",
    "train_features = sm.add_constant(train_features)\n",
    "dev_features = sm.add_constant(dev_features)\n",
    "test_features = sm.add_constant(test_features)\n",
    "\n",
    "# Train model\n",
    "mod = sm.OLS(train_labels, train_features, hasconst=True)\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "\n",
    "## Q3\n",
    "# Create unit change data\n",
    "data = pd.DataFrame([[15, 40, 1000, 70], [16, 40, 1000, 70]], columns=FEATURE_NAMES)\n",
    "unit_features = pd.DataFrame(scaler.transform(data), columns=FEATURE_NAMES)\n",
    "unit_features = sm.add_constant(unit_features, has_constant='add')\n",
    "\n",
    "# Predict target and assess change\n",
    "pred = res.predict(unit_features)\n",
    "print('\\nQ3 \\n Data:')\n",
    "print(unit_features)\n",
    "print('\\nEP effect with unit change on AT')\n",
    "print(pred)\n",
    "print('difference: ')\n",
    "print(pred[1]-pred[0])\n",
    "\n",
    "## Q4\n",
    "# Fetch data\n",
    "dev_samples = dev_features.head(2)\n",
    "weights = res.params\n",
    "\n",
    "print('\\nQ4 \\n Data:')\n",
    "print(dev_samples)\n",
    "print(weights)\n",
    "\n",
    "# Calculate feature effects\n",
    "dev_results = dev_samples.apply(lambda r: r * weights, axis=1)\n",
    "\n",
    "print('\\nResults:')\n",
    "print(dev_results)\n",
    "\n",
    "# Plot\n",
    "for idx, row in dev_results.iterrows():\n",
    "    plt.barh(FEATURE_NAMES, row[1:])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tj6Pri4HBeO"
   },
   "source": [
    "**Q5.** (10%) Reflection: why would training a regression tree not work well for this dataset in terms of model interpretability? (answer in the notebook)\n",
    "\n",
    "- Continous, zelfs als je 3 discrete waardes zou kiezen, zo de de boom alsnog enorm groot worden om alles te kunnen coveren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7r09mMfeo2k"
   },
   "source": [
    "## Training and Interpreting an Explainable Boosting Model (Generalized Additive Model)\n",
    "**Q6**. (10%) Train a Explainable Boosting Machine (with [interpret.ml](https://github.com/interpretml/interpret/))\n",
    "\n",
    "(Note on grading: Training EBM 4%, global and local explanation visualizations - see below -  3% each)\n",
    "\n",
    "For a tutorial see: [[Tutorial](https://nbviewer.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Interpretable%20Regression%20Methods.ipynb)]\n",
    "\n",
    "\n",
    "* (3%) Visualize/provide global (model-wise) feature importances for EBM as a table or figure. What are the most important two features in EBM? Are they the same as in the linear model?\n",
    "\n",
    "* (3%) Visualize local (instance-wise) feature importances for a development set instance of your choice.\n",
    "\n",
    "*Answer*\n",
    "Global feature importance is provided in code cell below. The most important features are:\n",
    "- AT\n",
    "- V\n",
    "\n",
    "This differs from the linear model, which suggested that RH was the second most important feature.\n",
    "\n",
    "Local feature importances are also provided below. Instances can be interactively chosen in the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-ZmqpxweoZv",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "\n",
    "# EBM\n",
    "ebm = ExplainableBoostingRegressor()\n",
    "ebm.fit(train_features, train_labels)\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)\n",
    "\n",
    "ebm_local = ebm.explain_local(dev_features, dev_labels)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k7dAwTIfbsc"
   },
   "source": [
    "## Training and Explaining Neural Networks\n",
    "**Q7**. (10%) Train a Neural Network (using the training and validation sets): One-layer MLP (with ReLU activation function and 50 to 100 hidden neurons) \n",
    "\n",
    "We recommend to use the Adam optimizer. Fine-tune the learning rate and any other hyper-parameters you find necessary. \n",
    "\n",
    "For a tutorial see: [[Tutorial](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)]\n",
    "\n",
    "Your code should report the results following the instructions below:\n",
    "\n",
    "Note on grading: training NN: 4%, answering below sub-questions 3% each.\n",
    "\n",
    "* (3%) Apply the trained neural network model on the test set and report Root Mean Square Error (RMSE) performance measure.\n",
    "\n",
    "* (3%) Analyzing factors influencing the neural network predictions. \n",
    "See the [Documentation](https://scikit-learn.org/stable/modules/partial_dependence.html) to use Partial Dependence Plot (PDP)  implementation in python. Use the trained one-layer MLP model to  Generate and report a bivariate PDP using 'AT' (Ambient Temperature) and 'V' (Exhaust Vacuum) features (Note: not two univariate PDPs but one bivariate PDP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQjg_qtCf_WD",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# One-layer MLP : you can use  learning_rate_init=0.001 to get a reasonable model, optimize other parameters by experimentation\n",
    "# We advice that you name variable for the mlp regressor model 'mlp_reg' so that it will be consistent \n",
    "# with the scripts to call your implementation of PFI later in Q8:\n",
    "mlp_reg= # ... your scripts here and below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Model-Agnostic Global Explanations for NN predictions\n",
    "\n",
    "**Permutation Feature Importance (PFI)**\n",
    "\n",
    "**Q8**. (20%) Implement the permutation feature importance algorithm using RMSE as the error function (%12). No existing libraries (save measuring RMSE) are allowed to use. We will implement it ourselves.\n",
    "- (%4) Visualize feature importances obtained by PFI for the NN (one-layer MLP) model you trained using a bar graph.\n",
    "- (%4) Reflection: What are the most important two features obtained by PFI for MLP model? How do these two features compare to the top two features from the Linear Model and the EBM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Use the below function signature and the parameters to implement PFI\n",
    "def PFI(X, labels, model, rmse):\n",
    "# Your scripts implementing PFI here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Do not modify this part (unless you change the variable name mlp_reg above)\n",
    "base_rmse = mean_squared_error(dev_labels, mlp_reg.predict(dev_features) , squared=False)\n",
    "results_df = PFI(dev_features, dev_labels, mlp_reg, base_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Your scripts to visualize results_df using a bar graph here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpGv6J6XlBQ4"
   },
   "source": [
    "### Generating Model-Agnostic Explanations for NN predictions\n",
    "You can check the tutorials for LIME explanations for neural networks \n",
    "[[LIME Tutorial](https://nbviewer.org/github/interpretml/interpret/blob/develop/examples/python/Explaining_Blackbox_Regressors.ipynb)]\n",
    "\n",
    "\n",
    "**Q9**. (10%) Provide LIME-based explanations for trained NN model's predictions on two randomly selected test set instances.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIo-o_lClJYQ",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Global explanations\n",
    "import graphviz\n",
    "from interpret import show\n",
    "\n",
    "# Local explanations (LIME)\n",
    "from interpret.blackbox import LimeTabular\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
